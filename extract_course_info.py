# -*- coding: utf-8 -*-
"""Extract_course_info.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UYIcoKd-ZHZkmeQZtRJ3oxpXnojD8p4c
"""

import pandas as pd
import requests

response = requests.get('https://penncoursereview.com/api/base/current/search/courses/?search=CIS')
print(response.text)

jjjson = response.json()

df = pd.DataFrame(jjjson)

df = df.sort_values('id').drop(columns = ['recommendation_score'])

df[df['id'].str.contains("0")]

crawl_list = []
term_name = "ABC"
for semester in range(2018, 2023):
  for term in term_name:
    crawl_list.append('https://penncoursereview.com/api/base/' + str(semester) + term + '/search/courses/?search=CIS')
 
crawl_list

# Use urllib.urlopen to crawl all pages in crawl_list, and store the response of the page
# in list pages 

pages = []

for url in crawl_list:
    page = url.split("/")[-4] #extract the term from the url
    print('Looking at file %s' % page)
    
    resp = requests.get(url)
    if (resp.status_code == 404):
      continue
    print(resp.text)
    #Save page and url for later use.
    pages.append(resp)

#make into json
jsons = []
for resp in pages: 
  jsonfile = resp.json()
  jsons.append(jsonfile)
jsons

#make json into pandasDF
temp = pd.DataFrame()
for file in jsons:
    data = pd.DataFrame(file)
    temp = temp.append(data, ignore_index = True)

temp_df = temp.append(df)
temp_df.sort_values("semester").head()

final_df = temp.append(df)

final_df['updated_id'] = final_df['id'].str.slice(stop=7)
final_df['updated_id'] = final_df['updated_id'] + "0"
final_df['course_number'] = final_df['updated_id'].str.rsplit(pat = "-", n = 1).str[-1].astype(int)

final_df = final_df[(final_df['course_number']>=5000) & (final_df['course_number']<7000)]

final_df = final_df.drop(columns = ['recommendation_score','id'])
final_df = final_df[ ['updated_id'] + [ col for col in final_df.columns if col != 'updated_id']]

final_df.sort_values("updated_id", ascending = False).head()

trimed_df = final_df

trimed_df['term'] = trimed_df['semester'].str[-1]
trimed_df['season'] = trimed_df['term'].apply(lambda x: 'Spring' if x == 'A' else 'Summer' if x == 'B' else 'Fall')
trimed_df['year'] = trimed_df['semester'].str.slice(stop=4)
trimed_df['all_year'] = trimed_df.groupby(['updated_id'])['year'].transform(lambda x : ','.join(x))
trimed_df.head()

trimed_df = pd.get_dummies(trimed_df, columns = ['season'])
#trimed_df =trimed_df[['updated_id']].join(pd.get_dummies(trimed_df, columns = ['season']).set_index('updated_id')).groupby('updated_id').max()
trimed_df = trimed_df.groupby('updated_id').max()

trimed_df = trimed_df.reset_index()

trimed_df.sort_values("updated_id", ascending = False).head()

trimed_df.loc[trimed_df['updated_id'] == 'CIS-5400']

import numpy as np

season_df = trimed_df.drop(columns = ['semester','num_sections','term','year'])
#season_df['description'].replace('', np.nan, inplace=True)
season_df = season_df.drop_duplicates(subset=['updated_id'])

season_df.sort_values("updated_id", ascending = False).head()

def flatten(x):
  new_list = []
  for sublist in x:
    for element in sublist:
        new_list.append(element)
  return new_list

from collections import Counter
def find_common(x):
  most_common_list = Counter(x).most_common(20)
  return most_common_list

!pip install git+https://github.com/LIAAD/yake
import yake

def keywords(full_text):
  kw_extractor = yake.KeywordExtractor(top=5, n=2)
  keywords = kw_extractor.extract_keywords(full_text)
  # for kw, v in keywords:
  #   print("Keyphrase: ",kw, ": score", v)
  return keywords

season_df['full_content'] = season_df['title'] + ". " + season_df['description']
season_df['full_content'] = season_df['full_content'].str.replace("CIS", "")
season_df.head()

season_df['keywords'] = season_df.apply(lambda x: keywords(x['full_content']), axis = 1)

season_df['keywords']

def first_in_tuple(x):
  words = []
  for element in x:
    words.append(element[0])
  return words

season_df['keywords_only'] = season_df.apply(lambda x: first_in_tuple(x['keywords']), axis = 1)

season_df.head()

####to use keybert
# !pip install keybert
# from keybert import KeyBERT

# kw_model = KeyBERT(model='all-mpnet-base-v2')

# def keywords_bert(full_text):
#   keywords = kw_model.extract_keywords(full_text, 

#                                       keyphrase_ngram_range=(1, 2), 

#                                       stop_words='english', 

#                                       highlight=False,

#                                       top_n=10)

#   keywords_list= list(dict(keywords).keys())
#   return keywords_list

# season_df['keywords_bert'] = season_df.apply(lambda x: keywords_bert(x['full_content']), axis = 1)
# season_df['keywords_bert']

table_df = season_df.drop(columns = ['full_content','keywords'])
table_df.rename(columns={'keywords_only':'keywords_yake', 'updated_id':'id'}, inplace=True)

keywords = table_df['keywords_yake'].values.tolist()

flattened_keywords = flatten(keywords)
most_common_list = find_common(flattened_keywords)
most_common_list

from wordcloud import WordCloud
import matplotlib.pyplot as plt 
#make a word cloud for top tokens (MANUALLY GRADED)
words = " ".join(flattened_keywords)
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                max_words = 100,
                colormap = 'twilight',
                min_font_size = 8).generate(words)


plt.figure(figsize = (8, 8))
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

from pymongo import MongoClient

# client = MongoClient("mongodb+srv://yilenda:18910010621@citcoursefinder.hg8kjgx.mongodb.net/?retryWrites=true&w=majority", connect=False )
# db = client['CITCourses']
# collection = db['Tabledf']
# data_dict = table_df.to_dict(orient='records')
# # Insert collection
# collection.insert_many(data_dict)

# data_dict

import json
result = table_df.to_json(orient="records")
parsed = json.loads(result)
json.dumps(parsed)